{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple models comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = boto3.client(service_name='forecast')\n",
    "forecastquery = boto3.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'util_power_forecastdemo' # This should be the same as your previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetGroupArn = 'arn:aws:forecast:eu-west-1:553700203877:dataset-group/util_power_forecastdemo_dsg'  # Fill in the quotes from the output of the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = ['Prophet', 'ETS', 'Deep_AR_Plus']\n",
    "\n",
    "predictors = {a:{} for a in algos}\n",
    "\n",
    "for p in predictors:\n",
    "    predictors[p]['predictor_name'] = project + '_' + p + '_algo'\n",
    "    predictors[p]['algorithm_arn'] = 'arn:aws:forecast:::algorithm/' + p\n",
    "\n",
    "pp.pprint(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastHorizon = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create multiple predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictor_response(pred_name, algo_arn, forecast_horizon):\n",
    "    response=forecast.create_predictor(PredictorName=pred_name, \n",
    "                                       AlgorithmArn=algo_arn,\n",
    "                                       ForecastHorizon=forecast_horizon,\n",
    "                                       PerformAutoML= False,\n",
    "                                       PerformHPO=False,\n",
    "                                       EvaluationParameters= {\"NumberOfBacktestWindows\": 1, \n",
    "                                                              \"BackTestWindowOffset\": 24}, \n",
    "                                       InputDataConfig= {\"DatasetGroupArn\": datasetGroupArn},\n",
    "                                       FeaturizationConfig= {\"ForecastFrequency\": \"H\", \n",
    "                                                             \"Featurizations\": \n",
    "                                                             [\n",
    "                                                                 {\"AttributeName\": \"target_value\", \n",
    "                                                                  \"FeaturizationPipeline\": \n",
    "                                                                  [\n",
    "                                                                      {\"FeaturizationMethodName\": \"filling\", \n",
    "                                                                       \"FeaturizationMethodParameters\": \n",
    "                                                                       {\"frontfill\": \"none\", \n",
    "                                                                        \"middlefill\": \"zero\", \n",
    "                                                                        \"backfill\": \"zero\"}\n",
    "                                                                      }\n",
    "                                                                  ]\n",
    "                                                                 }\n",
    "                                                             ]\n",
    "                                                            }\n",
    "                                      )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in predictors.keys():\n",
    "    predictor_response = create_predictor_response(predictors[p]['predictor_name'], predictors[p]['algorithm_arn'], forecastHorizon)\n",
    "    \n",
    "    predictorArn=predictor_response['PredictorArn']\n",
    "    \n",
    "    # wait for the predictor to be actually created\n",
    "    print('------------------ Creating ' + p)\n",
    "    while True:\n",
    "        predictorStatus = forecast.describe_predictor(PredictorArn=predictorArn)['Status']\n",
    "        print(predictorStatus)\n",
    "        if predictorStatus != 'ACTIVE' and predictorStatus != 'CREATE_FAILED':\n",
    "            sleep(30)\n",
    "        else:\n",
    "            predictors[p]['predictor_arn'] = predictorArn  # save it, just for reference\n",
    "            break\n",
    "            \n",
    "    # compute accuracy metrics, then proceed with the next algorithm        \n",
    "    predictors[p]['accuracy'] = forecast.get_accuracy_metrics(PredictorArn=predictorArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** (Bar?)plot RMSE, 0.9-, 0.5- and 0.1-quantile LossValues for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(columns=['predictor', 'RMSE'])\n",
    "for p in predictors:\n",
    "    score = predictors[p]['accuracy']['PredictorEvaluationResults'][0]['TestWindows'][0]['Metrics']['RMSE']\n",
    "    scores = scores.append(pd.DataFrame({'predictor':[p], 'RMSE':[score]}), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.barplot(data=scores, x='predictor', y='RMSE').set_title('Root Mean Square Error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rasa1",
   "language": "python",
   "name": "rasa1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
